
Evaluated model with following config: 
 {'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '16' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '128' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_users': '1475' 
, 'num_items': '232' 
, 'max_seq_length': '398' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=398, out_features=398, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=128, out_features=128, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=128, out_features=128, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
}
Evaluated model with following config: 
 {'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '16' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '128' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_users': '1475' 
, 'num_items': '232' 
, 'max_seq_length': '398' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=398, out_features=398, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=128, out_features=128, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=128, out_features=128, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
}
Evaluated model with following config: 
 {'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '16' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '128' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_users': '1475' 
, 'num_items': '232' 
, 'max_seq_length': '398' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=398, out_features=398, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=128, out_features=128, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=128, out_features=128, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
}
Evaluated model with following config: 
 {'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '16' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '128' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_users': '1475' 
, 'num_items': '232' 
, 'max_seq_length': '398' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=398, out_features=398, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=128, out_features=128, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=128, out_features=128, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
}
---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'epochs': '70' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 139357

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'epochs': '70' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 139357

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'epochs': '70' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=232, out_features=64, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=128, bias=False)
          (w_ks): Linear(in_features=64, out_features=128, bias=False)
          (w_vs): Linear(in_features=64, out_features=128, bias=False)
          (fc): Linear(in_features=128, out_features=64, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=32, bias=True)
          (w_2): Linear(in_features=32, out_features=64, bias=True)
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=14848, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '4' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
}

Number of parameters: 1029728

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'epochs': '70' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(232, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(232, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
, 'n_head': '4' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '232' 
}

Number of parameters: 112873

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '16' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'epochs': '70' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=398, out_features=398, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=128, out_features=128, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=128, out_features=128, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '4' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '232' 
, 'embedding_channels': '128' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_users': '1475' 
, 'num_items': '232' 
, 'max_seq_length': '398' 
}

Number of parameters: 220996

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'defi_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/defi_preprocessed/split_1/' 
, 'num_types': '39' 
, 'emb_dim': '32' 
, 'nhead': '8' 
, 'dropout': '0.3' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(39, 32)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
    )
    (linear1): Linear(in_features=64, out_features=64, bias=True)
    (dropout): Dropout(p=0.3, inplace=False)
    (linear2): Linear(in_features=64, out_features=64, bias=True)
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.3, inplace=False)
    (dropout2): Dropout(p=0.3, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
        )
        (linear1): Linear(in_features=64, out_features=64, bias=True)
        (dropout): Dropout(p=0.3, inplace=False)
        (linear2): Linear(in_features=64, out_features=64, bias=True)
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.3, inplace=False)
        (dropout2): Dropout(p=0.3, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=64, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 76961

Best validation roc auc on train: 0.813785729827754

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'defi_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/defi_preprocessed/split_1/' 
, 'num_types': '39' 
, 'emb_dim': '32' 
, 'nhead': '8' 
, 'dropout': '0.3' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(39, 32)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
    )
    (linear1): Linear(in_features=64, out_features=64, bias=True)
    (dropout): Dropout(p=0.3, inplace=False)
    (linear2): Linear(in_features=64, out_features=64, bias=True)
    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.3, inplace=False)
    (dropout2): Dropout(p=0.3, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
        )
        (linear1): Linear(in_features=64, out_features=64, bias=True)
        (dropout): Dropout(p=0.3, inplace=False)
        (linear2): Linear(in_features=64, out_features=64, bias=True)
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.3, inplace=False)
        (dropout2): Dropout(p=0.3, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=64, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 76961

---------------------------------------

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'defi_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.006' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '39' 
, 'data': 'tcmbn_data/defi_preprocessed/split_1/' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '64' 
, 'gau_comps': '32' 
, 'dropout': '0.1' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=39, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=2496, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
}

Number of parameters: 91120

Best validation roc auc on train: 0.8795704966371751

---------------------------------------

Evaluated model with following config

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'defi_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.006' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '39' 
, 'data': 'tcmbn_data/defi_preprocessed/split_1/' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '64' 
, 'gau_comps': '32' 
, 'dropout': '0.1' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=39, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=2496, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
}

Number of parameters: 91120

---------------------------------------

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 64)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (linear1): Linear(in_features=128, out_features=128, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=128, out_features=128, bias=True)
    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 309697

Best validation roc auc on train: 0.972608004018651

---------------------------------------

Evaluated model with following config

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 64)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (linear1): Linear(in_features=128, out_features=128, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=128, out_features=128, bias=True)
    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 309697

---------------------------------------

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.0015' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
, 'dropout': '0.1' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=169, out_features=64, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=32, bias=False)
          (w_ks): Linear(in_features=64, out_features=32, bias=False)
          (w_vs): Linear(in_features=64, out_features=32, bias=False)
          (fc): Linear(in_features=32, out_features=64, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=32, bias=True)
          (w_2): Linear(in_features=32, out_features=64, bias=True)
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=10816, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
}

Number of parameters: 739040

Best validation roc auc on train: 0.8485909316209472

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 64)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (linear1): Linear(in_features=128, out_features=128, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=128, out_features=128, bias=True)
    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 309697

Best validation roc auc on train: 0.9728776302548694

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 64)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (linear1): Linear(in_features=128, out_features=128, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=128, out_features=128, bias=True)
    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 309697

Best validation roc auc on train: 0.9730943763205823

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 64)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (linear1): Linear(in_features=128, out_features=128, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=128, out_features=128, bias=True)
    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 309697

Best validation roc auc on train: 0.9721194596332801

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.0015' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=169, out_features=64, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=32, bias=False)
          (w_ks): Linear(in_features=64, out_features=32, bias=False)
          (w_vs): Linear(in_features=64, out_features=32, bias=False)
          (fc): Linear(in_features=32, out_features=64, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=32, bias=True)
          (w_2): Linear(in_features=32, out_features=64, bias=True)
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=10816, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
}

Number of parameters: 739040

Best validation roc auc on train: 0.8737384703704544

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.0015' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=169, out_features=64, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=32, bias=False)
          (w_ks): Linear(in_features=64, out_features=32, bias=False)
          (w_vs): Linear(in_features=64, out_features=32, bias=False)
          (fc): Linear(in_features=32, out_features=64, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=32, bias=True)
          (w_2): Linear(in_features=32, out_features=64, bias=True)
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=10816, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
}

Number of parameters: 739040

Best validation roc auc on train: 0.8722793276172164

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.0015' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=169, out_features=64, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=32, bias=False)
          (w_ks): Linear(in_features=64, out_features=32, bias=False)
          (w_vs): Linear(in_features=64, out_features=32, bias=False)
          (fc): Linear(in_features=32, out_features=64, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=32, bias=True)
          (w_2): Linear(in_features=32, out_features=64, bias=True)
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=10816, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
}

Number of parameters: 739040

Best validation roc auc on train: 0.8693417862905244

---------------------------------------

Evaluated model with following config

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 64)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (linear1): Linear(in_features=128, out_features=128, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=128, out_features=128, bias=True)
    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 309697

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 64)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (linear1): Linear(in_features=128, out_features=128, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=128, out_features=128, bias=True)
    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 309697

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 64)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (linear1): Linear(in_features=128, out_features=128, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=128, out_features=128, bias=True)
    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 309697

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 64)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (linear1): Linear(in_features=128, out_features=128, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=128, out_features=128, bias=True)
    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 309697

---------------------------------------

Evaluated model with following config

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.0015' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=169, out_features=64, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=32, bias=False)
          (w_ks): Linear(in_features=64, out_features=32, bias=False)
          (w_vs): Linear(in_features=64, out_features=32, bias=False)
          (fc): Linear(in_features=32, out_features=64, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=32, bias=True)
          (w_2): Linear(in_features=32, out_features=64, bias=True)
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=10816, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
}

Number of parameters: 739040

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.0015' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=169, out_features=64, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=32, bias=False)
          (w_ks): Linear(in_features=64, out_features=32, bias=False)
          (w_vs): Linear(in_features=64, out_features=32, bias=False)
          (fc): Linear(in_features=32, out_features=64, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=32, bias=True)
          (w_2): Linear(in_features=32, out_features=64, bias=True)
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=10816, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
}

Number of parameters: 739040

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.0015' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=169, out_features=64, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=32, bias=False)
          (w_ks): Linear(in_features=64, out_features=32, bias=False)
          (w_vs): Linear(in_features=64, out_features=32, bias=False)
          (fc): Linear(in_features=32, out_features=64, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=32, bias=True)
          (w_2): Linear(in_features=32, out_features=64, bias=True)
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=10816, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
}

Number of parameters: 739040

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.0015' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=169, out_features=64, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=32, bias=False)
          (w_ks): Linear(in_features=64, out_features=32, bias=False)
          (w_vs): Linear(in_features=64, out_features=32, bias=False)
          (fc): Linear(in_features=32, out_features=64, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=32, bias=True)
          (w_2): Linear(in_features=32, out_features=64, bias=True)
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=10816, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
}

Number of parameters: 739040

---------------------------------------

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '128' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '384' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'num_users': '3980' 
, 'num_items': '169' 
, 'max_seq_length': '41' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=41, out_features=41, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=384, out_features=384, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=384, out_features=384, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
}

Number of parameters: 361873

Best validation roc auc on train: 0.7117701849077179

---------------------------------------

Evaluated model with following config

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '128' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '384' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'num_users': '3980' 
, 'num_items': '169' 
, 'max_seq_length': '41' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=41, out_features=41, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=384, out_features=384, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=384, out_features=384, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
}

Number of parameters: 361873

---------------------------------------

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '150' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.05' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'num_types': '169' 
, 'item_embed_dim': '256' 
, 'loss_function': 'multi_label_soft_loss' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10]' 
, 'items_total': '169' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(169, 256)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=256, out_features=256, bias=False)
    (Wk): Linear(in_features=256, out_features=256, bias=False)
    (Wv): Linear(in_features=256, out_features=256, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=256, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(169, 256)
  )
  (fc_output): Linear(in_features=256, out_features=1, bias=True)
)' 
}

Number of parameters: 373162

Best validation roc auc on train: 0.6970549603297337

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '128' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '384' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'num_users': '3980' 
, 'num_items': '169' 
, 'max_seq_length': '41' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=41, out_features=41, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=384, out_features=384, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=384, out_features=384, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
}

Number of parameters: 361873

Best validation roc auc on train: 0.7100114469822255

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '128' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '384' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'num_users': '3980' 
, 'num_items': '169' 
, 'max_seq_length': '41' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=41, out_features=41, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=384, out_features=384, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=384, out_features=384, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
}

Number of parameters: 361873

Best validation roc auc on train: 0.7060411410005685

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '128' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '384' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'num_users': '3980' 
, 'num_items': '169' 
, 'max_seq_length': '41' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=41, out_features=41, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=384, out_features=384, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=384, out_features=384, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
}

Number of parameters: 361873

Best validation roc auc on train: 0.7056213811944262

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.05' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '384' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'num_users': '3980' 
, 'num_items': '169' 
, 'max_seq_length': '41' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(169, 256)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=256, out_features=256, bias=False)
    (Wk): Linear(in_features=256, out_features=256, bias=False)
    (Wv): Linear(in_features=256, out_features=256, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=256, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(169, 256)
  )
  (fc_output): Linear(in_features=256, out_features=1, bias=True)
)' 
, 'num_types': '169' 
, 'item_embed_dim': '256' 
, 'loss_function': 'multi_label_soft_loss' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10]' 
, 'items_total': '169' 
}

Number of parameters: 373162

Best validation roc auc on train: 0.697372974723483

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.05' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '384' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'num_users': '3980' 
, 'num_items': '169' 
, 'max_seq_length': '41' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(169, 256)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=256, out_features=256, bias=False)
    (Wk): Linear(in_features=256, out_features=256, bias=False)
    (Wv): Linear(in_features=256, out_features=256, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=256, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(169, 256)
  )
  (fc_output): Linear(in_features=256, out_features=1, bias=True)
)' 
, 'num_types': '169' 
, 'item_embed_dim': '256' 
, 'loss_function': 'multi_label_soft_loss' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10]' 
, 'items_total': '169' 
}

Number of parameters: 373162

Best validation roc auc on train: 0.6980668996353147

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.05' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '384' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'num_users': '3980' 
, 'num_items': '169' 
, 'max_seq_length': '41' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(169, 256)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=256, out_features=256, bias=False)
    (Wk): Linear(in_features=256, out_features=256, bias=False)
    (Wv): Linear(in_features=256, out_features=256, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=256, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(169, 256)
  )
  (fc_output): Linear(in_features=256, out_features=1, bias=True)
)' 
, 'num_types': '169' 
, 'item_embed_dim': '256' 
, 'loss_function': 'multi_label_soft_loss' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10]' 
, 'items_total': '169' 
}

Number of parameters: 373162

Best validation roc auc on train: 0.697247296272439

---------------------------------------

Evaluated model with following config

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 64)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (linear1): Linear(in_features=128, out_features=128, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=128, out_features=128, bias=True)
    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 309697

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 64)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (linear1): Linear(in_features=128, out_features=128, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=128, out_features=128, bias=True)
    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 309697

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 64)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (linear1): Linear(in_features=128, out_features=128, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=128, out_features=128, bias=True)
    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 309697

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 64)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (linear1): Linear(in_features=128, out_features=128, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=128, out_features=128, bias=True)
    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (linear1): Linear(in_features=128, out_features=128, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=128, out_features=128, bias=True)
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=128, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 309697

---------------------------------------

Evaluated model with following config

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.0015' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=169, out_features=64, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=32, bias=False)
          (w_ks): Linear(in_features=64, out_features=32, bias=False)
          (w_vs): Linear(in_features=64, out_features=32, bias=False)
          (fc): Linear(in_features=32, out_features=64, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=32, bias=True)
          (w_2): Linear(in_features=32, out_features=64, bias=True)
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=10816, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
}

Number of parameters: 739040

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.0015' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=169, out_features=64, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=32, bias=False)
          (w_ks): Linear(in_features=64, out_features=32, bias=False)
          (w_vs): Linear(in_features=64, out_features=32, bias=False)
          (fc): Linear(in_features=32, out_features=64, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=32, bias=True)
          (w_2): Linear(in_features=32, out_features=64, bias=True)
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=10816, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
}

Number of parameters: 739040

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.0015' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=169, out_features=64, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=32, bias=False)
          (w_ks): Linear(in_features=64, out_features=32, bias=False)
          (w_vs): Linear(in_features=64, out_features=32, bias=False)
          (fc): Linear(in_features=32, out_features=64, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=32, bias=True)
          (w_2): Linear(in_features=32, out_features=64, bias=True)
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=10816, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
}

Number of parameters: 739040

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.0015' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=169, out_features=64, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=32, bias=False)
          (w_ks): Linear(in_features=64, out_features=32, bias=False)
          (w_vs): Linear(in_features=64, out_features=32, bias=False)
          (fc): Linear(in_features=32, out_features=64, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=32, bias=True)
          (w_2): Linear(in_features=32, out_features=64, bias=True)
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=10816, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
}

Number of parameters: 739040

---------------------------------------

Evaluated model with following config

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '128' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=41, out_features=41, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=384, out_features=384, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=384, out_features=384, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
, 'embedding_channels': '384' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'num_users': '3980' 
, 'num_items': '169' 
, 'max_seq_length': '41' 
}

Number of parameters: 361873

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '128' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=41, out_features=41, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=384, out_features=384, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=384, out_features=384, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
, 'embedding_channels': '384' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'num_users': '3980' 
, 'num_items': '169' 
, 'max_seq_length': '41' 
}

Number of parameters: 361873

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '128' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=41, out_features=41, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=384, out_features=384, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=384, out_features=384, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
, 'embedding_channels': '384' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'num_users': '3980' 
, 'num_items': '169' 
, 'max_seq_length': '41' 
}

Number of parameters: 361873

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '128' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=41, out_features=41, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=384, out_features=384, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=384, out_features=384, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
, 'embedding_channels': '384' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'num_users': '3980' 
, 'num_items': '169' 
, 'max_seq_length': '41' 
}

Number of parameters: 361873

---------------------------------------

Evaluated model with following config

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.05' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(169, 256)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=256, out_features=256, bias=False)
    (Wk): Linear(in_features=256, out_features=256, bias=False)
    (Wv): Linear(in_features=256, out_features=256, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=256, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(169, 256)
  )
  (fc_output): Linear(in_features=256, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
, 'embedding_channels': '384' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'num_users': '3980' 
, 'num_items': '169' 
, 'max_seq_length': '41' 
, 'item_embed_dim': '256' 
, 'loss_function': 'multi_label_soft_loss' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10]' 
, 'items_total': '169' 
}

Number of parameters: 373162

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.05' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(169, 256)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=256, out_features=256, bias=False)
    (Wk): Linear(in_features=256, out_features=256, bias=False)
    (Wv): Linear(in_features=256, out_features=256, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=256, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(169, 256)
  )
  (fc_output): Linear(in_features=256, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
, 'embedding_channels': '384' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'num_users': '3980' 
, 'num_items': '169' 
, 'max_seq_length': '41' 
, 'item_embed_dim': '256' 
, 'loss_function': 'multi_label_soft_loss' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10]' 
, 'items_total': '169' 
}

Number of parameters: 373162

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.05' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(169, 256)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=256, out_features=256, bias=False)
    (Wk): Linear(in_features=256, out_features=256, bias=False)
    (Wv): Linear(in_features=256, out_features=256, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=256, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(169, 256)
  )
  (fc_output): Linear(in_features=256, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
, 'embedding_channels': '384' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'num_users': '3980' 
, 'num_items': '169' 
, 'max_seq_length': '41' 
, 'item_embed_dim': '256' 
, 'loss_function': 'multi_label_soft_loss' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10]' 
, 'items_total': '169' 
}

Number of parameters: 373162

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.05' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '64' 
, 'nhead': '4' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(169, 256)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=256, out_features=256, bias=False)
    (Wk): Linear(in_features=256, out_features=256, bias=False)
    (Wv): Linear(in_features=256, out_features=256, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=256, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(169, 256)
  )
  (fc_output): Linear(in_features=256, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
, 'embedding_channels': '384' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'num_users': '3980' 
, 'num_items': '169' 
, 'max_seq_length': '41' 
, 'item_embed_dim': '256' 
, 'loss_function': 'multi_label_soft_loss' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10]' 
, 'items_total': '169' 
}

Number of parameters: 373162

---------------------------------------

{'seed': '5' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.0015' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '64' 
, 'd_inner': '32' 
, 'd_k': '32' 
, 'd_v': '32' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
, 'dropout': '0.1' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=169, out_features=64, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=64, out_features=32, bias=False)
          (w_ks): Linear(in_features=64, out_features=32, bias=False)
          (w_vs): Linear(in_features=64, out_features=32, bias=False)
          (fc): Linear(in_features=32, out_features=64, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=64, out_features=32, bias=True)
          (w_2): Linear(in_features=32, out_features=64, bias=True)
          (layer_norm): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=64, out_features=10816, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
}

Number of parameters: 739040

Best validation roc auc on train: 0.8644295660468952

---------------------------------------

{'seed': '5' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'dunnhumby_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '24' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '24' 
, 'data': 'tcmbn_data/dunnhumby_preprocessed/split_1/' 
, 'emb_dim': '40' 
, 'nhead': '5' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '1' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(24, 40)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=80, out_features=80, bias=True)
    )
    (linear1): Linear(in_features=80, out_features=80, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=80, out_features=80, bias=True)
    (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=80, out_features=80, bias=True)
        )
        (linear1): Linear(in_features=80, out_features=80, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=80, out_features=80, bias=True)
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=80, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 79441

Best validation roc auc on train: 0.7362983958429435

---------------------------------------

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'dunnhumby_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '24' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '24' 
, 'data': 'tcmbn_data/dunnhumby_preprocessed/split_1/' 
, 'emb_dim': '36' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(24, 36)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
    )
    (linear1): Linear(in_features=72, out_features=72, bias=True)
    (dropout): Dropout(p=0.2, inplace=False)
    (linear2): Linear(in_features=72, out_features=72, bias=True)
    (norm1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
        )
        (linear1): Linear(in_features=72, out_features=72, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (linear2): Linear(in_features=72, out_features=72, bias=True)
        (norm1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.2, inplace=False)
        (dropout2): Dropout(p=0.2, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=72, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 96409

Best validation roc auc on train: 0.7434775127489716

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'dunnhumby_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '24' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '24' 
, 'data': 'tcmbn_data/dunnhumby_preprocessed/split_1/' 
, 'emb_dim': '36' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(24, 36)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
    )
    (linear1): Linear(in_features=72, out_features=72, bias=True)
    (dropout): Dropout(p=0.2, inplace=False)
    (linear2): Linear(in_features=72, out_features=72, bias=True)
    (norm1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
        )
        (linear1): Linear(in_features=72, out_features=72, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (linear2): Linear(in_features=72, out_features=72, bias=True)
        (norm1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.2, inplace=False)
        (dropout2): Dropout(p=0.2, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=72, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 96409

Best validation roc auc on train: 0.7422061983550646

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'dunnhumby_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '24' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '24' 
, 'data': 'tcmbn_data/dunnhumby_preprocessed/split_1/' 
, 'emb_dim': '36' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(24, 36)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
    )
    (linear1): Linear(in_features=72, out_features=72, bias=True)
    (dropout): Dropout(p=0.2, inplace=False)
    (linear2): Linear(in_features=72, out_features=72, bias=True)
    (norm1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
        )
        (linear1): Linear(in_features=72, out_features=72, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (linear2): Linear(in_features=72, out_features=72, bias=True)
        (norm1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.2, inplace=False)
        (dropout2): Dropout(p=0.2, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=72, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 96409

Best validation roc auc on train: 0.7500195712866381

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'dunnhumby_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '24' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '24' 
, 'data': 'tcmbn_data/dunnhumby_preprocessed/split_1/' 
, 'emb_dim': '36' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(24, 36)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
    )
    (linear1): Linear(in_features=72, out_features=72, bias=True)
    (dropout): Dropout(p=0.2, inplace=False)
    (linear2): Linear(in_features=72, out_features=72, bias=True)
    (norm1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.2, inplace=False)
    (dropout2): Dropout(p=0.2, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
        )
        (linear1): Linear(in_features=72, out_features=72, bias=True)
        (dropout): Dropout(p=0.2, inplace=False)
        (linear2): Linear(in_features=72, out_features=72, bias=True)
        (norm1): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((72,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.2, inplace=False)
        (dropout2): Dropout(p=0.2, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=72, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 96409

Best validation roc auc on train: 0.7431670175906434

---------------------------------------

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'dunnhumby_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '24' 
, 'data': 'tcmbn_data/dunnhumby_preprocessed/split_1/' 
, 'emb_dim': '36' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=24, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=96, bias=False)
          (w_ks): Linear(in_features=32, out_features=96, bias=False)
          (w_vs): Linear(in_features=32, out_features=96, bias=False)
          (fc): Linear(in_features=96, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=1536, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '6' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
}

Number of parameters: 69200

Best validation roc auc on train: 0.7704352757863938

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'dunnhumby_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '24' 
, 'data': 'tcmbn_data/dunnhumby_preprocessed/split_1/' 
, 'emb_dim': '36' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=24, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=96, bias=False)
          (w_ks): Linear(in_features=32, out_features=96, bias=False)
          (w_vs): Linear(in_features=32, out_features=96, bias=False)
          (fc): Linear(in_features=96, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=1536, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '6' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
}

Number of parameters: 69200

Best validation roc auc on train: 0.7695605426363913

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'dunnhumby_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '24' 
, 'data': 'tcmbn_data/dunnhumby_preprocessed/split_1/' 
, 'emb_dim': '36' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=24, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=96, bias=False)
          (w_ks): Linear(in_features=32, out_features=96, bias=False)
          (w_vs): Linear(in_features=32, out_features=96, bias=False)
          (fc): Linear(in_features=96, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=1536, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '6' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
}

Number of parameters: 69200

Best validation roc auc on train: 0.7771960872777796

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'dunnhumby_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '24' 
, 'data': 'tcmbn_data/dunnhumby_preprocessed/split_1/' 
, 'emb_dim': '36' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=24, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=96, bias=False)
          (w_ks): Linear(in_features=32, out_features=96, bias=False)
          (w_vs): Linear(in_features=32, out_features=96, bias=False)
          (fc): Linear(in_features=96, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=64, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=1536, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '6' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
}

Number of parameters: 69200

Best validation roc auc on train: 0.7640060429461041

---------------------------------------

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'dunnhumby_preprocessed' 
, 'cuda': '0' 
, 'epoch': '500' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '24' 
, 'data': 'tcmbn_data/dunnhumby_preprocessed/split_1/' 
, 'emb_dim': '36' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=520, out_features=520, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '6' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/dunnhumby_preprocessed/split_1/' 
, 'num_users': '1497' 
, 'num_items': '24' 
, 'max_seq_length': '520' 
}

Number of parameters: 280192

Best validation roc auc on train: 0.5408799743457531

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'dunnhumby_preprocessed' 
, 'cuda': '0' 
, 'epoch': '500' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '24' 
, 'data': 'tcmbn_data/dunnhumby_preprocessed/split_1/' 
, 'emb_dim': '36' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=520, out_features=520, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '6' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '64' 
, 'gau_comps': '64' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/dunnhumby_preprocessed/split_1/' 
, 'num_users': '1497' 
, 'num_items': '24' 
, 'max_seq_length': '520' 
}

Number of parameters: 280192

Best validation roc auc on train: 0.5688909244820108

---------------------------------------

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '217' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'dropout': '0.1' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=217, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=6944, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
}

Number of parameters: 242544

Best validation roc auc on train: 0.9005389455692179

---------------------------------------

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'epochs': '50' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(217, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 138727

Best validation roc auc on train: 0.9893120382842844

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'epochs': '50' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(217, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 138727

Best validation roc auc on train: 0.9910516448151595

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'epochs': '50' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(217, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 138727

Best validation roc auc on train: 0.9902161184880663

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'epochs': '50' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(217, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 138727

Best validation roc auc on train: 0.989551893939775

---------------------------------------

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '70' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'epochs': '50' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=217, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=6944, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
}

Number of parameters: 242544

Best validation roc auc on train: 0.9005389455692179

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '70' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'epochs': '50' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=217, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=6944, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
}

Number of parameters: 242544

Best validation roc auc on train: 0.9011059120639395

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '70' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'epochs': '50' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=217, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=6944, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
}

Number of parameters: 242544

Best validation roc auc on train: 0.8969485110053207

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '70' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'epochs': '50' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=217, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=6944, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
}

Number of parameters: 242544

Best validation roc auc on train: 0.9025753809530027

---------------------------------------

Evaluated model with following config

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(217, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 138727

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(217, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 138727

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(217, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 138727

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(217, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 138727

---------------------------------------

Evaluated model with following config

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '70' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=217, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=6944, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
}

Number of parameters: 242544

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '70' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=217, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=6944, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
}

Number of parameters: 242544

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '70' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=217, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=6944, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
}

Number of parameters: 242544

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '70' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=217, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=6944, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
}

Number of parameters: 242544

---------------------------------------

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(134, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 135241

Best validation roc auc on train: 0.9125348953916913

---------------------------------------

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=134, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=4288, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
}

Number of parameters: 152240

Best validation roc auc on train: 0.8144454521121623

---------------------------------------

Evaluated model with following config

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(134, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 135241

---------------------------------------

Evaluated model with following config

{'seed': '0' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=134, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=4288, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
}

Number of parameters: 152240

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(134, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 135241

Best validation roc auc on train: 0.9120328617156733

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(134, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 135241

Best validation roc auc on train: 0.9118747519909851

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(134, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 135241

Best validation roc auc on train: 0.912288712369152

---------------------------------------

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(134, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 135241

Best validation roc auc on train: 0.9133927151080996

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=134, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=4288, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
}

Number of parameters: 152240

Best validation roc auc on train: 0.8181226228428474

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=134, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=4288, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
}

Number of parameters: 152240

Best validation roc auc on train: 0.817246105943617

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=134, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=4288, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
}

Number of parameters: 152240

Best validation roc auc on train: 0.8215668713600935

---------------------------------------

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=134, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=4288, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
}

Number of parameters: 152240

Best validation roc auc on train: 0.811635971652141

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=99, out_features=99, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
}

Number of parameters: 26633

Best validation roc auc on train: 0.7092641637194

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=99, out_features=99, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
}

Number of parameters: 26633

Best validation roc auc on train: 0.7049791558871918

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=99, out_features=99, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
}

Number of parameters: 26633

Best validation roc auc on train: 0.709978459629897

---------------------------------------

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=99, out_features=99, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
}

Number of parameters: 26633

Best validation roc auc on train: 0.6122445069383197

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(217, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(217, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 110938

Best validation roc auc on train: 0.7891598637368609

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(217, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(217, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 110938

Best validation roc auc on train: 0.7886240445201707

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(217, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(217, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 110938

Best validation roc auc on train: 0.7886475000422374

---------------------------------------

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(217, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(217, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 110938

Best validation roc auc on train: 0.7890967216756709

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(217, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 138727

Best validation roc auc on train: 0.9910516448151595

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(217, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 138727

Best validation roc auc on train: 0.9902161184880663

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(217, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 138727

Best validation roc auc on train: 0.989551893939775

---------------------------------------

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(217, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 138727

Best validation roc auc on train: 0.9881288406878113

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '70' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=217, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=6944, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 242544

Best validation roc auc on train: 0.9011059120639395

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '70' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=217, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=6944, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 242544

Best validation roc auc on train: 0.8969485110053207

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '70' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=217, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=6944, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 242544

Best validation roc auc on train: 0.9025753809530027

---------------------------------------

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '70' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=217, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=6944, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 242544

Best validation roc auc on train: 0.8916039598820479

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=99, out_features=99, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 26633

Best validation roc auc on train: 0.7092641637194

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=99, out_features=99, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 26633

Best validation roc auc on train: 0.7049791558871918

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=99, out_features=99, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 26633

Best validation roc auc on train: 0.709978459629897

---------------------------------------

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=99, out_features=99, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 26633

Best validation roc auc on train: 0.6122445069383197

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(217, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(217, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 110938

Best validation roc auc on train: 0.655934617454188

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(217, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(217, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 110938

Best validation roc auc on train: 0.6559322610700423

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(217, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(217, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 110938

Best validation roc auc on train: 0.6559789919161912

---------------------------------------

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(217, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(217, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 110938

Best validation roc auc on train: 0.6559129690317548

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(134, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 135241

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(134, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 135241

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(134, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 135241

---------------------------------------

Evaluated model with following config

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(134, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 135241

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=134, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=4288, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
}

Number of parameters: 152240

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=134, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=4288, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
}

Number of parameters: 152240

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=134, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=4288, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
}

Number of parameters: 152240

---------------------------------------

Evaluated model with following config

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=134, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=4288, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
}

Number of parameters: 152240

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=99, out_features=99, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
}

Number of parameters: 26633

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=99, out_features=99, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
}

Number of parameters: 26633

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=99, out_features=99, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
}

Number of parameters: 26633

---------------------------------------

Evaluated model with following config

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=99, out_features=99, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
}

Number of parameters: 26633

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(217, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(217, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 110938

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(217, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(217, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 110938

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(217, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(217, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 110938

---------------------------------------

Evaluated model with following config

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(217, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(217, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 110938

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(217, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 138727

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(217, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 138727

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(217, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 138727

---------------------------------------

Evaluated model with following config

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(217, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 138727

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '70' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=217, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=6944, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 242544

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '70' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=217, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=6944, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 242544

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '70' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=217, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=6944, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 242544

---------------------------------------

Evaluated model with following config

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'TCMBN' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '70' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.002' 
, 'batch_size': '32' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'Transformer(
  (encoder): Encoder(
    (event_emb): Linear(in_features=217, out_features=32, bias=False)
    (layer_stack): ModuleList(
      (0): EncoderLayer(
        (slf_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=32, out_features=16, bias=False)
          (w_ks): Linear(in_features=32, out_features=16, bias=False)
          (w_vs): Linear(in_features=32, out_features=16, bias=False)
          (fc): Linear(in_features=16, out_features=32, bias=True)
          (attention): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (pos_ffn): PositionwiseFeedForward(
          (w_1): Linear(in_features=32, out_features=16, bias=True)
          (w_2): Linear(in_features=16, out_features=32, bias=True)
          (layer_norm): LayerNorm((32,), eps=1e-06, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (MBN): MixtureBernoulliNetwork(
    (pi_network): CategoricalNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=32, bias=True)
      )
    )
    (bernoulli_network): BernoulliNetwork(
      (network): Sequential(
        (0): Linear(in_features=32, out_features=32, bias=True)
        (1): ELU(alpha=1.0)
        (2): Linear(in_features=32, out_features=6944, bias=True)
        (3): Sigmoid()
      )
    )
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 242544

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=99, out_features=99, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 26633

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=99, out_features=99, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 26633

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=99, out_features=99, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 26633

---------------------------------------

Evaluated model with following config

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=99, out_features=99, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 26633

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(217, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(217, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 110938

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(217, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(217, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 110938

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(217, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(217, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 110938

---------------------------------------

Evaluated model with following config

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_types': '217' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.2' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'False' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(217, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(217, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
, 'n_head': '1' 
, 'n_layers': '1' 
, 'd_model': '32' 
, 'd_inner': '16' 
, 'd_k': '16' 
, 'd_v': '16' 
, 'ber_comps': '32' 
, 'gau_comps': '32' 
, 'embedding_channels': '64' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_users': '3000' 
, 'num_items': '134' 
, 'max_seq_length': '99' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '217' 
}

Number of parameters: 110938

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '64' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_users': '15875' 
, 'num_items': '217' 
, 'max_seq_length': '19' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=19, out_features=19, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
}

Number of parameters: 22505

Best validation roc auc on train: 0.6777528476037313

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '64' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_users': '15875' 
, 'num_items': '217' 
, 'max_seq_length': '19' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=19, out_features=19, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
}

Number of parameters: 22505

Best validation roc auc on train: 0.6769960319238736

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '64' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_users': '15875' 
, 'num_items': '217' 
, 'max_seq_length': '19' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=19, out_features=19, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
}

Number of parameters: 22505

Best validation roc auc on train: 0.6791844464902757

---------------------------------------

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '64' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_users': '15875' 
, 'num_items': '217' 
, 'max_seq_length': '19' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=19, out_features=19, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
}

Number of parameters: 22505

Best validation roc auc on train: 0.6789509792278992

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '64' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_users': '15875' 
, 'num_items': '217' 
, 'max_seq_length': '19' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=19, out_features=19, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
}

Number of parameters: 22505

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '64' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_users': '15875' 
, 'num_items': '217' 
, 'max_seq_length': '19' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=19, out_features=19, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
}

Number of parameters: 22505

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '64' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_users': '15875' 
, 'num_items': '217' 
, 'max_seq_length': '19' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=19, out_features=19, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
}

Number of parameters: 22505

---------------------------------------

Evaluated model with following config

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'SFCNTSP' 
, 'dataset_name': 'DC_preprocessed' 
, 'cuda': '0' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'bias': 'False' 
, 'embedding_channels': '64' 
, 'dropout': '0.2' 
, 'scheduler_t_max': '1000' 
, 'alpha': '1.0' 
, 'beta': '0.1' 
, 'data_path': 'tcmbn_data/DC_preprocessed/split_1/' 
, 'num_users': '15875' 
, 'num_items': '217' 
, 'max_seq_length': '19' 
, 'model': 'SFCNTSP(
  (dropout): Dropout(p=0.2, inplace=False)
  (temporal_dependencies): TemporalDependencies(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=19, out_features=19, bias=False)
  )
  (element_relationships): ElementRelationships(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (channel_correlations): ChannelCorrelations(
    (dropout): Dropout(p=0.2, inplace=False)
    (sfcn): Linear(in_features=64, out_features=64, bias=False)
  )
  (representations_fusing): RepresentationsFusing(
    (dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=64, out_features=64, bias=True)
    (leaky_relu_func): LeakyReLU(negative_slope=0.2)
  )
)' 
}

Number of parameters: 22505

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '134' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(134, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(134, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
}

Number of parameters: 100231

Best validation roc auc on train: 0.7906361530627389

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '134' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(134, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(134, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
}

Number of parameters: 100231

Best validation roc auc on train: 0.7900347976114342

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '134' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(134, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(134, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
}

Number of parameters: 100231

Best validation roc auc on train: 0.7899413176106453

---------------------------------------

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '134' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(134, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(134, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
}

Number of parameters: 100231

Best validation roc auc on train: 0.7898271548827251

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '134' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(134, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(134, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
}

Number of parameters: 100231

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '134' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(134, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(134, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
}

Number of parameters: 100231

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '134' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(134, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(134, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
}

Number of parameters: 100231

---------------------------------------

Evaluated model with following config

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'DNNTSP' 
, 'dataset_name': 'instacart_preprocessed' 
, 'cuda': '0' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.01' 
, 'batch_size': '64' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/instacart_preprocessed/split_1/' 
, 'num_types': '134' 
, 'item_embed_dim': '128' 
, 'loss_function': 'multi_label_soft_loss' 
, 'learning_rate': '0.001' 
, 'weight_decay': '0' 
, 'top_k': '[5, 10, 20]' 
, 'items_total': '134' 
, 'model': 'temporal_set_prediction(
  (item_embedding): Embedding(134, 128)
  (stacked_gcn): stacked_weighted_GCN_blocks(
    (0): weighted_GCN(
      (gcns): ModuleList(
        (0-1): 2 x weighted_graph_conv(
          (linear): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (relus): ModuleList(
        (0-1): 2 x ReLU()
      )
      (bns): ModuleList(
        (0-1): 2 x BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (masked_self_attention): masked_self_attention(
    (Wq): Linear(in_features=128, out_features=128, bias=False)
    (Wk): Linear(in_features=128, out_features=128, bias=False)
    (Wv): Linear(in_features=128, out_features=128, bias=False)
  )
  (aggregate_nodes_temporal_feature): aggregate_nodes_temporal_feature(
    (Wq): Linear(in_features=128, out_features=1, bias=False)
  )
  (global_gated_update): global_gated_update(
    (item_embedding): Embedding(134, 128)
  )
  (fc_output): Linear(in_features=128, out_features=1, bias=True)
)' 
}

Number of parameters: 100231

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '1' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '4' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 4)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
    )
    (linear1): Linear(in_features=8, out_features=8, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=8, out_features=8, bias=True)
    (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
        )
        (linear1): Linear(in_features=8, out_features=8, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=8, out_features=8, bias=True)
        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 2077

Best validation roc auc on train: 0.9639896308905549

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '1' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '4' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 4)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
    )
    (linear1): Linear(in_features=8, out_features=8, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=8, out_features=8, bias=True)
    (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
        )
        (linear1): Linear(in_features=8, out_features=8, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=8, out_features=8, bias=True)
        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 2077

Best validation roc auc on train: 0.9639896308905549

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '1' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '4' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 4)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
    )
    (linear1): Linear(in_features=8, out_features=8, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=8, out_features=8, bias=True)
    (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
        )
        (linear1): Linear(in_features=8, out_features=8, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=8, out_features=8, bias=True)
        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 2077

Best validation roc auc on train: 0.9616180146295925

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '1' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '4' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 4)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
    )
    (linear1): Linear(in_features=8, out_features=8, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=8, out_features=8, bias=True)
    (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
        )
        (linear1): Linear(in_features=8, out_features=8, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=8, out_features=8, bias=True)
        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 2077

Best validation roc auc on train: 0.9612868335555923

---------------------------------------

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '1' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '4' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 4)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
    )
    (linear1): Linear(in_features=8, out_features=8, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=8, out_features=8, bias=True)
    (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
        )
        (linear1): Linear(in_features=8, out_features=8, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=8, out_features=8, bias=True)
        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 2077

Best validation roc auc on train: 0.9633877942609963

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '2' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '8' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 8)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
    )
    (linear1): Linear(in_features=16, out_features=16, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=16, out_features=16, bias=True)
    (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
        )
        (linear1): Linear(in_features=16, out_features=16, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=16, out_features=16, bias=True)
        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=16, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 6457

Best validation roc auc on train: 0.9739778484402726

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '2' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '8' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 8)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
    )
    (linear1): Linear(in_features=16, out_features=16, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=16, out_features=16, bias=True)
    (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
        )
        (linear1): Linear(in_features=16, out_features=16, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=16, out_features=16, bias=True)
        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=16, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 6457

Best validation roc auc on train: 0.9762636657986397

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '2' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '8' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 8)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
    )
    (linear1): Linear(in_features=16, out_features=16, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=16, out_features=16, bias=True)
    (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
        )
        (linear1): Linear(in_features=16, out_features=16, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=16, out_features=16, bias=True)
        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=16, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 6457

Best validation roc auc on train: 0.9733938434250645

---------------------------------------

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '2' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '8' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 8)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
    )
    (linear1): Linear(in_features=16, out_features=16, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=16, out_features=16, bias=True)
    (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
        )
        (linear1): Linear(in_features=16, out_features=16, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=16, out_features=16, bias=True)
        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=16, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 6457

Best validation roc auc on train: 0.9728356872254101

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '2' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '8' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 8)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
    )
    (linear1): Linear(in_features=16, out_features=16, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=16, out_features=16, bias=True)
    (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
        )
        (linear1): Linear(in_features=16, out_features=16, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=16, out_features=16, bias=True)
        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=16, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 6457

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '2' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '8' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 8)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
    )
    (linear1): Linear(in_features=16, out_features=16, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=16, out_features=16, bias=True)
    (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
        )
        (linear1): Linear(in_features=16, out_features=16, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=16, out_features=16, bias=True)
        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=16, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 6457

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '2' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '8' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 8)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
    )
    (linear1): Linear(in_features=16, out_features=16, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=16, out_features=16, bias=True)
    (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
        )
        (linear1): Linear(in_features=16, out_features=16, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=16, out_features=16, bias=True)
        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=16, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 6457

---------------------------------------

Evaluated model with following config

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '2' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '8' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 8)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
    )
    (linear1): Linear(in_features=16, out_features=16, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=16, out_features=16, bias=True)
    (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
        )
        (linear1): Linear(in_features=16, out_features=16, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=16, out_features=16, bias=True)
        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=16, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 6457

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '1' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '4' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 4)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
    )
    (linear1): Linear(in_features=8, out_features=8, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=8, out_features=8, bias=True)
    (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
        )
        (linear1): Linear(in_features=8, out_features=8, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=8, out_features=8, bias=True)
        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 2077

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '1' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '4' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 4)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
    )
    (linear1): Linear(in_features=8, out_features=8, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=8, out_features=8, bias=True)
    (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
        )
        (linear1): Linear(in_features=8, out_features=8, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=8, out_features=8, bias=True)
        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 2077

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '1' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '4' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 4)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
    )
    (linear1): Linear(in_features=8, out_features=8, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=8, out_features=8, bias=True)
    (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
        )
        (linear1): Linear(in_features=8, out_features=8, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=8, out_features=8, bias=True)
        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 2077

---------------------------------------

Evaluated model with following config

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '1' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '4' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 4)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
    )
    (linear1): Linear(in_features=8, out_features=8, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=8, out_features=8, bias=True)
    (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)
        )
        (linear1): Linear(in_features=8, out_features=8, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=8, out_features=8, bias=True)
        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=8, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 2077

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '3' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '12' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 12)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
    )
    (linear1): Linear(in_features=24, out_features=24, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=24, out_features=24, bias=True)
    (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
        )
        (linear1): Linear(in_features=24, out_features=24, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=24, out_features=24, bias=True)
        (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=24, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 13141

Best validation roc auc on train: 0.9758182921711338

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '3' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '12' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 12)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
    )
    (linear1): Linear(in_features=24, out_features=24, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=24, out_features=24, bias=True)
    (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
        )
        (linear1): Linear(in_features=24, out_features=24, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=24, out_features=24, bias=True)
        (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=24, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 13141

Best validation roc auc on train: 0.9764050952379234

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '3' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '12' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 12)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
    )
    (linear1): Linear(in_features=24, out_features=24, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=24, out_features=24, bias=True)
    (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
        )
        (linear1): Linear(in_features=24, out_features=24, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=24, out_features=24, bias=True)
        (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=24, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 13141

Best validation roc auc on train: 0.9759562003503937

---------------------------------------

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '3' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '12' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 12)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
    )
    (linear1): Linear(in_features=24, out_features=24, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=24, out_features=24, bias=True)
    (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
        )
        (linear1): Linear(in_features=24, out_features=24, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=24, out_features=24, bias=True)
        (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=24, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 13141

Best validation roc auc on train: 0.9749331739380189

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '3' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '12' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 12)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
    )
    (linear1): Linear(in_features=24, out_features=24, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=24, out_features=24, bias=True)
    (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
        )
        (linear1): Linear(in_features=24, out_features=24, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=24, out_features=24, bias=True)
        (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=24, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 13141

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '3' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '12' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 12)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
    )
    (linear1): Linear(in_features=24, out_features=24, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=24, out_features=24, bias=True)
    (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
        )
        (linear1): Linear(in_features=24, out_features=24, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=24, out_features=24, bias=True)
        (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=24, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 13141

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '3' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '12' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 12)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
    )
    (linear1): Linear(in_features=24, out_features=24, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=24, out_features=24, bias=True)
    (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
        )
        (linear1): Linear(in_features=24, out_features=24, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=24, out_features=24, bias=True)
        (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=24, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 13141

---------------------------------------

Evaluated model with following config

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'mimic3_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '3' 
, 'epoch': '20' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '8' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '10' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'num_types': '169' 
, 'data': 'tcmbn_data/mimic3_preprocessed/split_1/' 
, 'emb_dim': '12' 
, 'nhead': '4' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '1e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(169, 12)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
    )
    (linear1): Linear(in_features=24, out_features=24, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=24, out_features=24, bias=True)
    (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
        )
        (linear1): Linear(in_features=24, out_features=24, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=24, out_features=24, bias=True)
        (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=24, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 13141

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '1' 
, 'epoch': '100' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '6' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'epochs': '70' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 6)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
    )
    (linear1): Linear(in_features=12, out_features=12, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=12, out_features=12, bias=True)
    (norm1): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=12, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=12, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=12, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 4357

Best validation roc auc on train: 0.8621520764115019

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '1' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '6' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'epochs': '70' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 6)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
    )
    (linear1): Linear(in_features=12, out_features=12, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=12, out_features=12, bias=True)
    (norm1): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=12, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=12, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=12, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 4357

Best validation roc auc on train: 0.8677175826235226

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '1' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '6' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'epochs': '70' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 6)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
    )
    (linear1): Linear(in_features=12, out_features=12, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=12, out_features=12, bias=True)
    (norm1): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=12, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=12, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=12, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 4357

Best validation roc auc on train: 0.8665088370217952

---------------------------------------

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '1' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '6' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'epochs': '70' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 6)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
    )
    (linear1): Linear(in_features=12, out_features=12, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=12, out_features=12, bias=True)
    (norm1): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=12, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=12, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=12, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 4357

Best validation roc auc on train: 0.8610015496039176

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '1' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '6' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 6)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
    )
    (linear1): Linear(in_features=12, out_features=12, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=12, out_features=12, bias=True)
    (norm1): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=12, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=12, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=12, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 4357

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '1' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '6' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 6)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
    )
    (linear1): Linear(in_features=12, out_features=12, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=12, out_features=12, bias=True)
    (norm1): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=12, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=12, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=12, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 4357

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '1' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '6' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 6)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
    )
    (linear1): Linear(in_features=12, out_features=12, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=12, out_features=12, bias=True)
    (norm1): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=12, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=12, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=12, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 4357

---------------------------------------

Evaluated model with following config

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '1' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '6' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 6)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
    )
    (linear1): Linear(in_features=12, out_features=12, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=12, out_features=12, bias=True)
    (norm1): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=12, out_features=12, bias=True)
        )
        (linear1): Linear(in_features=12, out_features=12, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=12, out_features=12, bias=True)
        (norm1): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((12,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=12, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 4357

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '2' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '12' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 12)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
    )
    (linear1): Linear(in_features=24, out_features=24, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=24, out_features=24, bias=True)
    (norm1): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
        )
        (linear1): Linear(in_features=24, out_features=24, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=24, out_features=24, bias=True)
        (norm1): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=24, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 13897

Best validation roc auc on train: 0.8738398894752912

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '2' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '12' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 12)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
    )
    (linear1): Linear(in_features=24, out_features=24, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=24, out_features=24, bias=True)
    (norm1): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
        )
        (linear1): Linear(in_features=24, out_features=24, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=24, out_features=24, bias=True)
        (norm1): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=24, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 13897

Best validation roc auc on train: 0.8774121754405231

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '2' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '12' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 12)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
    )
    (linear1): Linear(in_features=24, out_features=24, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=24, out_features=24, bias=True)
    (norm1): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
        )
        (linear1): Linear(in_features=24, out_features=24, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=24, out_features=24, bias=True)
        (norm1): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=24, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 13897

Best validation roc auc on train: 0.8765986679532677

---------------------------------------

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '2' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '12' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 12)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
    )
    (linear1): Linear(in_features=24, out_features=24, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=24, out_features=24, bias=True)
    (norm1): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
        )
        (linear1): Linear(in_features=24, out_features=24, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=24, out_features=24, bias=True)
        (norm1): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=24, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 13897

Best validation roc auc on train: 0.8818284820089928

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '2' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '12' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 12)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
    )
    (linear1): Linear(in_features=24, out_features=24, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=24, out_features=24, bias=True)
    (norm1): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
        )
        (linear1): Linear(in_features=24, out_features=24, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=24, out_features=24, bias=True)
        (norm1): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=24, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 13897

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '2' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '12' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 12)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
    )
    (linear1): Linear(in_features=24, out_features=24, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=24, out_features=24, bias=True)
    (norm1): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
        )
        (linear1): Linear(in_features=24, out_features=24, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=24, out_features=24, bias=True)
        (norm1): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=24, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 13897

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '2' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '12' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 12)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
    )
    (linear1): Linear(in_features=24, out_features=24, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=24, out_features=24, bias=True)
    (norm1): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
        )
        (linear1): Linear(in_features=24, out_features=24, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=24, out_features=24, bias=True)
        (norm1): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=24, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 13897

---------------------------------------

Evaluated model with following config

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '2' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '12' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 12)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
    )
    (linear1): Linear(in_features=24, out_features=24, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=24, out_features=24, bias=True)
    (norm1): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)
        )
        (linear1): Linear(in_features=24, out_features=24, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=24, out_features=24, bias=True)
        (norm1): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((24,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=24, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 13897

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '3' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '18' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 18)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
    )
    (linear1): Linear(in_features=36, out_features=36, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=36, out_features=36, bias=True)
    (norm1): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
        )
        (linear1): Linear(in_features=36, out_features=36, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=36, out_features=36, bias=True)
        (norm1): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=36, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 28621

Best validation roc auc on train: 0.8760705307928796

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '3' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '18' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 18)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
    )
    (linear1): Linear(in_features=36, out_features=36, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=36, out_features=36, bias=True)
    (norm1): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
        )
        (linear1): Linear(in_features=36, out_features=36, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=36, out_features=36, bias=True)
        (norm1): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=36, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 28621

Best validation roc auc on train: 0.8788628903681643

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '3' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '18' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 18)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
    )
    (linear1): Linear(in_features=36, out_features=36, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=36, out_features=36, bias=True)
    (norm1): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
        )
        (linear1): Linear(in_features=36, out_features=36, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=36, out_features=36, bias=True)
        (norm1): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=36, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 28621

Best validation roc auc on train: 0.8711425598818259

---------------------------------------

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '3' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '18' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 18)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
    )
    (linear1): Linear(in_features=36, out_features=36, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=36, out_features=36, bias=True)
    (norm1): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
        )
        (linear1): Linear(in_features=36, out_features=36, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=36, out_features=36, bias=True)
        (norm1): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=36, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 28621

Best validation roc auc on train: 0.8792275043536769

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '3' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '18' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 18)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
    )
    (linear1): Linear(in_features=36, out_features=36, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=36, out_features=36, bias=True)
    (norm1): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
        )
        (linear1): Linear(in_features=36, out_features=36, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=36, out_features=36, bias=True)
        (norm1): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=36, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 28621

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '3' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '18' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 18)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
    )
    (linear1): Linear(in_features=36, out_features=36, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=36, out_features=36, bias=True)
    (norm1): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
        )
        (linear1): Linear(in_features=36, out_features=36, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=36, out_features=36, bias=True)
        (norm1): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=36, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 28621

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '3' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '18' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 18)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
    )
    (linear1): Linear(in_features=36, out_features=36, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=36, out_features=36, bias=True)
    (norm1): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
        )
        (linear1): Linear(in_features=36, out_features=36, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=36, out_features=36, bias=True)
        (norm1): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=36, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 28621

---------------------------------------

Evaluated model with following config

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '3' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '18' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 18)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
    )
    (linear1): Linear(in_features=36, out_features=36, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=36, out_features=36, bias=True)
    (norm1): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)
        )
        (linear1): Linear(in_features=36, out_features=36, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=36, out_features=36, bias=True)
        (norm1): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((36,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=36, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 28621

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '4' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '24' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 24)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
    )
    (linear1): Linear(in_features=48, out_features=48, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=48, out_features=48, bias=True)
    (norm1): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
        )
        (linear1): Linear(in_features=48, out_features=48, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=48, out_features=48, bias=True)
        (norm1): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=48, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 48529

Best validation roc auc on train: 0.8690412144846396

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '4' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '24' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 24)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
    )
    (linear1): Linear(in_features=48, out_features=48, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=48, out_features=48, bias=True)
    (norm1): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
        )
        (linear1): Linear(in_features=48, out_features=48, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=48, out_features=48, bias=True)
        (norm1): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=48, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 48529

Best validation roc auc on train: 0.8693876580637738

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '4' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '24' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 24)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
    )
    (linear1): Linear(in_features=48, out_features=48, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=48, out_features=48, bias=True)
    (norm1): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
        )
        (linear1): Linear(in_features=48, out_features=48, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=48, out_features=48, bias=True)
        (norm1): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=48, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 48529

Best validation roc auc on train: 0.875287209409278

---------------------------------------

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '4' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '24' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 24)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
    )
    (linear1): Linear(in_features=48, out_features=48, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=48, out_features=48, bias=True)
    (norm1): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
        )
        (linear1): Linear(in_features=48, out_features=48, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=48, out_features=48, bias=True)
        (norm1): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=48, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 48529

Best validation roc auc on train: 0.8690430122226671

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '4' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '24' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 24)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
    )
    (linear1): Linear(in_features=48, out_features=48, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=48, out_features=48, bias=True)
    (norm1): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
        )
        (linear1): Linear(in_features=48, out_features=48, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=48, out_features=48, bias=True)
        (norm1): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=48, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 48529

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '4' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '24' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 24)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
    )
    (linear1): Linear(in_features=48, out_features=48, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=48, out_features=48, bias=True)
    (norm1): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
        )
        (linear1): Linear(in_features=48, out_features=48, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=48, out_features=48, bias=True)
        (norm1): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=48, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 48529

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '4' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '24' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 24)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
    )
    (linear1): Linear(in_features=48, out_features=48, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=48, out_features=48, bias=True)
    (norm1): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
        )
        (linear1): Linear(in_features=48, out_features=48, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=48, out_features=48, bias=True)
        (norm1): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=48, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 48529

---------------------------------------

Evaluated model with following config

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '4' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '24' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 24)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
    )
    (linear1): Linear(in_features=48, out_features=48, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=48, out_features=48, bias=True)
    (norm1): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)
        )
        (linear1): Linear(in_features=48, out_features=48, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=48, out_features=48, bias=True)
        (norm1): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((48,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=48, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 48529

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '5' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '36' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 36)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
    )
    (linear1): Linear(in_features=72, out_features=72, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=72, out_features=72, bias=True)
    (norm1): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
        )
        (linear1): Linear(in_features=72, out_features=72, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=72, out_features=72, bias=True)
        (norm1): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=72, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 103897

Best validation roc auc on train: 0.8784647120145768

---------------------------------------

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '5' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '36' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 36)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
    )
    (linear1): Linear(in_features=72, out_features=72, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=72, out_features=72, bias=True)
    (norm1): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
        )
        (linear1): Linear(in_features=72, out_features=72, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=72, out_features=72, bias=True)
        (norm1): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=72, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 103897

Best validation roc auc on train: 0.861251658011467

---------------------------------------

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '5' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '36' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 36)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
    )
    (linear1): Linear(in_features=72, out_features=72, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=72, out_features=72, bias=True)
    (norm1): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
        )
        (linear1): Linear(in_features=72, out_features=72, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=72, out_features=72, bias=True)
        (norm1): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=72, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 103897

Best validation roc auc on train: 0.86819945219242

---------------------------------------

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '5' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '36' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 36)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
    )
    (linear1): Linear(in_features=72, out_features=72, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=72, out_features=72, bias=True)
    (norm1): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
        )
        (linear1): Linear(in_features=72, out_features=72, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=72, out_features=72, bias=True)
        (norm1): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=72, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 103897

Best validation roc auc on train: 0.8630584803687124

---------------------------------------

Evaluated model with following config

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '5' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '36' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 36)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
    )
    (linear1): Linear(in_features=72, out_features=72, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=72, out_features=72, bias=True)
    (norm1): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
        )
        (linear1): Linear(in_features=72, out_features=72, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=72, out_features=72, bias=True)
        (norm1): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=72, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 103897

---------------------------------------

Evaluated model with following config

{'seed': '2' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '5' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '36' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 36)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
    )
    (linear1): Linear(in_features=72, out_features=72, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=72, out_features=72, bias=True)
    (norm1): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
        )
        (linear1): Linear(in_features=72, out_features=72, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=72, out_features=72, bias=True)
        (norm1): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=72, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 103897

---------------------------------------

Evaluated model with following config

{'seed': '3' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '5' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '36' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 36)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
    )
    (linear1): Linear(in_features=72, out_features=72, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=72, out_features=72, bias=True)
    (norm1): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
        )
        (linear1): Linear(in_features=72, out_features=72, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=72, out_features=72, bias=True)
        (norm1): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=72, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 103897

---------------------------------------

Evaluated model with following config

{'seed': '4' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '5' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '36' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 36)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
    )
    (linear1): Linear(in_features=72, out_features=72, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=72, out_features=72, bias=True)
    (norm1): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=72, out_features=72, bias=True)
        )
        (linear1): Linear(in_features=72, out_features=72, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=72, out_features=72, bias=True)
        (norm1): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((72,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=72, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 103897

---------------------------------------

{'seed': '1' 
, 'device': 'cuda:0' 
, 'model_name': 'LANET' 
, 'dataset_name': 'synthea_preprocessed' 
, 'cuda': '0' 
, 'ablation': 'True' 
, 'ablation_index': '6' 
, 'epoch': '50' 
, 'early_stop_thr': '3e-05' 
, 'patience': '15' 
, 'lr': '0.001' 
, 'batch_size': '4' 
, 'eps': '1e-05' 
, 'betas': '[0.9, 0.999]' 
, 'scheduler_step': '15' 
, 'gamma': '0.9' 
, 'log': 'log.txt' 
, 'data': 'tcmbn_data/synthea_preprocessed/split_1/' 
, 'num_types': '232' 
, 'emb_dim': '42' 
, 'nhead': '6' 
, 'dropout': '0.1' 
, 'activation': 'relu' 
, 'batch_first': 'True' 
, 'encoder_num_layers': '2' 
, 'layer_norm_eps': '2e-05' 
, 'bias': 'True' 
, 'model': 'TransformerEncoder(
  (cat_embedding): Embedding(232, 42)
  (encoder_layer): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
    )
    (linear1): Linear(in_features=84, out_features=84, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (linear2): Linear(in_features=84, out_features=84, bias=True)
    (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
    (dropout1): Dropout(p=0.1, inplace=False)
    (dropout2): Dropout(p=0.1, inplace=False)
  )
  (transformer_encoder): TransformerEncoder(
    (layers): ModuleList(
      (0-1): 2 x TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=84, out_features=84, bias=True)
        )
        (linear1): Linear(in_features=84, out_features=84, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=84, out_features=84, bias=True)
        (norm1): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (norm2): LayerNorm((84,), eps=2e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (dropout): Dropout(p=0.3, inplace=False)
  (encoder_history): Linear(in_features=84, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)' 
}

Number of parameters: 139357

Best validation roc auc on train: 0.8646366900895218
